<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <title>Teachable Machine Image Model</title>
</head>
<body>
<div class="container">
<h2>Teachable Machine Image Model</h2>
<label>Model url:</label><input type="text" id="modelUrl" value="https://teachablemachine.withgoogle.com/models/DT8NSD58T/" size="50"/>
<br/>
<button type="button" onclick="init()">Start</button>
<div id="webcam-container"></div>
<h3>Output</h3>
<div>
    <label>Play a sound when the class changes:</label>
    <input type="checkbox" id="classChangeSound" />
    <br/>
    <label>Instrument: </label>
    <select id="instrument">
<option value="guitar-acoustic"> guitar-acoustic </option>
<option value="guitar-electric"> guitar-electric </option>
<option value="harp">harp</option>
<option value="organ">organ</option>
<option value="piano" selected>piano</option>
    </select>
    <br/>
    <label for="pitch">Pitch (from low to high)</label>
<input type="range" id="pitch" name="pitch" min="0" max="35" step="1">
<br/>
<label for="pitchStep">Notes to step</label>
<input type="range" id="pitchStep" name="pitchStep" min="1" max="5" step="1" value="1">
</div>
<div id="label-container"></div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tone/14.7.56/Tone.js" integrity="sha512-s5zatYPJj3QNve/aStVSAP4Y7/+44yX2wDYpa37YouB3WjKbQTXtK/eKhw24F6uVpaHofetU66ZeXFsyQnXjRA==" crossorigin="anonymous"></script>
<script type="text/javascript" src="/js/Tonejs-Instruments.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8/dist/teachablemachine-image.min.js"></script>
<script type="text/javascript">
    // More API functions here:
    // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/pose
    // the link to your model provided by Teachable Machine export panel
    let model, webcam, ctx, labelContainer, maxPredictions, previousClass, samples, current;

    previousClass = -1;

    async function init() {
        const URL = document.getElementById("modelUrl").value;
        const modelURL = URL + "model.json";
        const metadataURL = URL + "metadata.json";



        samples = SampleLibrary.load({
            instruments: ['guitar-acoustic', 'guitar-electric', 'piano', 'organ', 'harp'],
            baseUrl: "/samples/"
        });

                    // loop through instruments and set release, connect to master output
        for (var property in samples) {
            if (samples.hasOwnProperty(property)) {
                    console.log(samples[property])
                    samples[property].release = .5;
                    samples[property].toDestination();
            }
        }

        current = samples['piano'];

	    await Tone.start();
	    console.log('audio is ready');

        // load the model and metadata
        // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
        // Note: the pose library adds a tmPose object to your window (window.tmPose)
        model = await tmImage.load(modelURL, metadataURL);
        maxPredictions = model.getTotalClasses();

        // Convenience function to setup a webcam
        const size = 200;
        const flip = true; // whether to flip the webcam
        webcam = new tmImage.Webcam(size, size, flip); // width, height, flip
        await webcam.setup(); // request access to the webcam
        await webcam.play();
        window.requestAnimationFrame(loop);

        // append/get elements to the DOM
        document.getElementById("webcam-container").appendChild(webcam.canvas);
        labelContainer = document.getElementById("label-container");
        for (let i = 0; i < maxPredictions; i++) { // and class labels
            labelContainer.appendChild(document.createElement("div"));
        }
    }

    async function loop(timestamp) {
        webcam.update(); // update the webcam frame
        await predict();
        window.requestAnimationFrame(loop);
    }

    async function predict() {
        // Prediction 2: run input through teachable machine classification model
        const prediction = await model.predict(webcam.canvas);

        let pMax = 0;
        let iMax = 0;
        for (let i = 0; i < maxPredictions; i++) {
            const classPrediction =
                prediction[i].className + ": " + prediction[i].probability.toFixed(2);
            if (prediction[i].probability > pMax) {
                iMax = i;
                pMax = prediction[i].probability;
            }
            labelContainer.childNodes[i].innerHTML = classPrediction;
        }



        if (document.getElementById("classChangeSound").checked) {
            if (iMax != previousClass) {
                current = samples[document.getElementById("instrument").value]
                const initialPitch = parseInt(document.getElementById("pitch").value);
                const pitchStep = parseInt(document.getElementById("pitchStep").value);
                console.log(pitchStep*iMax+initialPitch+25)
                current.triggerAttackRelease(Tone.Frequency(pitchStep*iMax+initialPitch+25, "midi").toNote(), '2n');
                previousClass = iMax;
            }
        }
    }

</script>

</body>
</html>